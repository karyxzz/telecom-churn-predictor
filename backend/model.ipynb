{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3329d69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d24c91d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload CSV safely\n",
    "df = pd.read_csv(\"preprocessed_churn.csv\")\n",
    "\n",
    "# Convert all columns except target to numeric\n",
    "for col in df.columns:\n",
    "    if col != \"Churn\":\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# Fill any accidental NaNs (from coercion)\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Split features/target\n",
    "X = df.drop(\"Churn\", axis=1).astype(np.float32)\n",
    "y = df[\"Churn\"].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ad83f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69aff086",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChurnDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4abbba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ChurnDataset(X_train, y_train)\n",
    "val_dataset   = ChurnDataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e27d1c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = y_train.value_counts().to_dict()\n",
    "weights = [1.0 / class_counts[int(label)] for label in y_train]\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33b1d015",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=128, sampler=sampler)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "455b1276",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChurnANN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(32, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.SELU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(16, 8),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.SELU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(8, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        # He initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55f41686",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ChurnANN(input_dim=X.shape[1]).to(device)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    def forward(self, inputs, targets):\n",
    "        inputs = torch.clamp(inputs, 1e-8, 1-1e-8)\n",
    "        bce = F.binary_cross_entropy(inputs, targets, reduction=\"none\")\n",
    "        pt = torch.where(targets == 1, inputs, 1 - inputs)\n",
    "        loss = self.alpha * (1 - pt) ** self.gamma * bce\n",
    "        return loss.mean()\n",
    "\n",
    "criterion = FocalLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR\n",
    "warmup_scheduler = LinearLR(optimizer, start_factor=0.1, total_iters=5)\n",
    "cosine_scheduler = CosineAnnealingLR(optimizer, T_max=20)\n",
    "scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51f2c545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: TrainLoss=0.4323, ValLoss=0.2573, ROC_AUC=0.4921, PR_AUC=0.2653, F1=0.3810\n",
      "Epoch 2: TrainLoss=0.3496, ValLoss=0.2003, ROC_AUC=0.6450, PR_AUC=0.3651, F1=0.4851\n",
      "Epoch 3: TrainLoss=0.2862, ValLoss=0.1690, ROC_AUC=0.7312, PR_AUC=0.4371, F1=0.5260\n",
      "Epoch 4: TrainLoss=0.2334, ValLoss=0.1638, ROC_AUC=0.7675, PR_AUC=0.4820, F1=0.5361\n",
      "Epoch 5: TrainLoss=0.2076, ValLoss=0.1539, ROC_AUC=0.7836, PR_AUC=0.5128, F1=0.5512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Churn Prediction\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:209: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: TrainLoss=0.2021, ValLoss=0.1488, ROC_AUC=0.7980, PR_AUC=0.5412, F1=0.5630\n",
      "Epoch 7: TrainLoss=0.1863, ValLoss=0.1474, ROC_AUC=0.7998, PR_AUC=0.5481, F1=0.5591\n",
      "Epoch 8: TrainLoss=0.1799, ValLoss=0.1469, ROC_AUC=0.8069, PR_AUC=0.5574, F1=0.5635\n",
      "Epoch 9: TrainLoss=0.1777, ValLoss=0.1447, ROC_AUC=0.8102, PR_AUC=0.5658, F1=0.5671\n",
      "Epoch 10: TrainLoss=0.1666, ValLoss=0.1424, ROC_AUC=0.8121, PR_AUC=0.5680, F1=0.5699\n",
      "Epoch 11: TrainLoss=0.1638, ValLoss=0.1408, ROC_AUC=0.8134, PR_AUC=0.5705, F1=0.5756\n",
      "Epoch 12: TrainLoss=0.1617, ValLoss=0.1419, ROC_AUC=0.8132, PR_AUC=0.5685, F1=0.5740\n",
      "Epoch 13: TrainLoss=0.1602, ValLoss=0.1405, ROC_AUC=0.8153, PR_AUC=0.5726, F1=0.5777\n",
      "Epoch 14: TrainLoss=0.1566, ValLoss=0.1365, ROC_AUC=0.8176, PR_AUC=0.5763, F1=0.5701\n",
      "Epoch 15: TrainLoss=0.1554, ValLoss=0.1373, ROC_AUC=0.8177, PR_AUC=0.5756, F1=0.5760\n",
      "Epoch 16: TrainLoss=0.1553, ValLoss=0.1397, ROC_AUC=0.8187, PR_AUC=0.5768, F1=0.5788\n",
      "Epoch 17: TrainLoss=0.1526, ValLoss=0.1379, ROC_AUC=0.8194, PR_AUC=0.5778, F1=0.5762\n",
      "Epoch 18: TrainLoss=0.1488, ValLoss=0.1389, ROC_AUC=0.8194, PR_AUC=0.5778, F1=0.5759\n",
      "Epoch 19: TrainLoss=0.1504, ValLoss=0.1387, ROC_AUC=0.8202, PR_AUC=0.5792, F1=0.5770\n",
      "Epoch 20: TrainLoss=0.1488, ValLoss=0.1380, ROC_AUC=0.8212, PR_AUC=0.5803, F1=0.5770\n",
      "Epoch 21: TrainLoss=0.1487, ValLoss=0.1370, ROC_AUC=0.8202, PR_AUC=0.5779, F1=0.5781\n",
      "Epoch 22: TrainLoss=0.1473, ValLoss=0.1384, ROC_AUC=0.8212, PR_AUC=0.5783, F1=0.5736\n",
      "Epoch 23: TrainLoss=0.1537, ValLoss=0.1368, ROC_AUC=0.8206, PR_AUC=0.5796, F1=0.5747\n",
      "Epoch 24: TrainLoss=0.1436, ValLoss=0.1358, ROC_AUC=0.8206, PR_AUC=0.5787, F1=0.5807\n",
      "Epoch 25: TrainLoss=0.1494, ValLoss=0.1367, ROC_AUC=0.8212, PR_AUC=0.5781, F1=0.5774\n",
      "Epoch 26: TrainLoss=0.1515, ValLoss=0.1393, ROC_AUC=0.8214, PR_AUC=0.5790, F1=0.5770\n",
      "Epoch 27: TrainLoss=0.1520, ValLoss=0.1378, ROC_AUC=0.8213, PR_AUC=0.5801, F1=0.5776\n",
      "Epoch 28: TrainLoss=0.1518, ValLoss=0.1371, ROC_AUC=0.8216, PR_AUC=0.5803, F1=0.5760\n",
      "Epoch 29: TrainLoss=0.1489, ValLoss=0.1386, ROC_AUC=0.8218, PR_AUC=0.5802, F1=0.5736\n",
      "Epoch 30: TrainLoss=0.1514, ValLoss=0.1375, ROC_AUC=0.8217, PR_AUC=0.5793, F1=0.5744\n",
      "Epoch 31: TrainLoss=0.1519, ValLoss=0.1375, ROC_AUC=0.8222, PR_AUC=0.5825, F1=0.5779\n",
      "Epoch 32: TrainLoss=0.1503, ValLoss=0.1362, ROC_AUC=0.8222, PR_AUC=0.5821, F1=0.5774\n",
      "Epoch 33: TrainLoss=0.1504, ValLoss=0.1354, ROC_AUC=0.8222, PR_AUC=0.5827, F1=0.5744\n",
      "Epoch 34: TrainLoss=0.1471, ValLoss=0.1372, ROC_AUC=0.8222, PR_AUC=0.5843, F1=0.5802\n",
      "Epoch 35: TrainLoss=0.1464, ValLoss=0.1385, ROC_AUC=0.8229, PR_AUC=0.5852, F1=0.5796\n",
      "Epoch 36: TrainLoss=0.1456, ValLoss=0.1342, ROC_AUC=0.8227, PR_AUC=0.5848, F1=0.5840\n",
      "Epoch 37: TrainLoss=0.1438, ValLoss=0.1352, ROC_AUC=0.8234, PR_AUC=0.5857, F1=0.5841\n",
      "Epoch 38: TrainLoss=0.1442, ValLoss=0.1345, ROC_AUC=0.8243, PR_AUC=0.5892, F1=0.5829\n",
      "Epoch 39: TrainLoss=0.1418, ValLoss=0.1362, ROC_AUC=0.8255, PR_AUC=0.5909, F1=0.5868\n",
      "Epoch 40: TrainLoss=0.1402, ValLoss=0.1336, ROC_AUC=0.8262, PR_AUC=0.5932, F1=0.5917\n",
      "Epoch 41: TrainLoss=0.1444, ValLoss=0.1321, ROC_AUC=0.8260, PR_AUC=0.5945, F1=0.5893\n",
      "Epoch 42: TrainLoss=0.1408, ValLoss=0.1336, ROC_AUC=0.8266, PR_AUC=0.5948, F1=0.5895\n",
      "Epoch 43: TrainLoss=0.1343, ValLoss=0.1339, ROC_AUC=0.8270, PR_AUC=0.5945, F1=0.5910\n",
      "Epoch 44: TrainLoss=0.1369, ValLoss=0.1343, ROC_AUC=0.8274, PR_AUC=0.5959, F1=0.5843\n",
      "Epoch 45: TrainLoss=0.1388, ValLoss=0.1310, ROC_AUC=0.8286, PR_AUC=0.5986, F1=0.5914\n",
      "Epoch 46: TrainLoss=0.1378, ValLoss=0.1351, ROC_AUC=0.8295, PR_AUC=0.5990, F1=0.5883\n",
      "Epoch 47: TrainLoss=0.1378, ValLoss=0.1311, ROC_AUC=0.8292, PR_AUC=0.6001, F1=0.5890\n",
      "Epoch 48: TrainLoss=0.1355, ValLoss=0.1335, ROC_AUC=0.8290, PR_AUC=0.6018, F1=0.5891\n",
      "Epoch 49: TrainLoss=0.1335, ValLoss=0.1345, ROC_AUC=0.8286, PR_AUC=0.6015, F1=0.5899\n",
      "Epoch 50: TrainLoss=0.1322, ValLoss=0.1348, ROC_AUC=0.8294, PR_AUC=0.6038, F1=0.5903\n",
      "Epoch 51: TrainLoss=0.1316, ValLoss=0.1333, ROC_AUC=0.8295, PR_AUC=0.6052, F1=0.5890\n",
      "Epoch 52: TrainLoss=0.1287, ValLoss=0.1365, ROC_AUC=0.8301, PR_AUC=0.6057, F1=0.5871\n",
      "Epoch 53: TrainLoss=0.1289, ValLoss=0.1332, ROC_AUC=0.8303, PR_AUC=0.6066, F1=0.5918\n",
      "Epoch 54: TrainLoss=0.1296, ValLoss=0.1331, ROC_AUC=0.8308, PR_AUC=0.6059, F1=0.5935\n",
      "Epoch 55: TrainLoss=0.1296, ValLoss=0.1341, ROC_AUC=0.8309, PR_AUC=0.6077, F1=0.5897\n",
      "Early stopping triggered.\n",
      "Best threshold: 0.566\n",
      "Final Accuracy (best threshold): 0.7889\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=100, patience=10):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        val_losses, y_true, y_pred = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch).squeeze()\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_losses.append(loss.item())\n",
    "                y_true.extend(y_batch.cpu().numpy())\n",
    "                y_pred.extend(outputs.cpu().numpy())\n",
    "\n",
    "        val_loss = np.mean(val_losses)\n",
    "        roc_auc = roc_auc_score(y_true, y_pred)\n",
    "        pr_auc  = average_precision_score(y_true, y_pred)\n",
    "        f1      = f1_score(y_true, (np.array(y_pred) > 0.5).astype(int))\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: TrainLoss={np.mean(train_losses):.4f}, ValLoss={val_loss:.4f}, ROC_AUC={roc_auc:.4f}, PR_AUC={pr_auc:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            y_true.extend(y_batch.cpu().numpy())\n",
    "            y_pred.extend(outputs.cpu().numpy())\n",
    "\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_curve\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    f1_scores = 2*precisions*recalls/(precisions+recalls+1e-8)\n",
    "    best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "    print(f\"Best threshold: {best_threshold:.3f}\")\n",
    "    final_preds = (np.array(y_pred) > best_threshold).astype(int)\n",
    "    final_acc = accuracy_score(y_true, final_preds)\n",
    "    print(f\"Final Accuracy (best threshold): {final_acc:.4f}\")\n",
    "\n",
    "train_model(model, train_loader, val_loader, epochs=200, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c116643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7889\n",
      "Validation ROC-AUC: 0.8286\n",
      "Validation PR-AUC: 0.5986\n",
      "Validation F1: 0.6207\n",
      "\n",
      "Confusion Matrix:\n",
      " [[867 166]\n",
      " [131 243]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.84      0.85      1033\n",
      "         1.0       0.59      0.65      0.62       374\n",
      "\n",
      "    accuracy                           0.79      1407\n",
      "   macro avg       0.73      0.74      0.74      1407\n",
      "weighted avg       0.80      0.79      0.79      1407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Model Evaluation on Validation Data\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_recall_curve\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "y_true, y_pred_probs = [], []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch).squeeze()\n",
    "        y_true.extend(y_batch.cpu().numpy())\n",
    "        y_pred_probs.extend(outputs.cpu().numpy())\n",
    "\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred_probs)\n",
    "f1_scores = 2*precisions*recalls/(precisions+recalls+1e-8)\n",
    "best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "y_pred = (np.array(y_pred_probs) > best_threshold).astype(int)\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "roc = roc_auc_score(y_true, y_pred_probs)\n",
    "pr  = average_precision_score(y_true, y_pred_probs)\n",
    "f1  = f1_score(y_true, y_pred)\n",
    "cm  = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "print(f\"Validation ROC-AUC: {roc:.4f}\")\n",
    "print(f\"Validation PR-AUC: {pr:.4f}\")\n",
    "print(f\"Validation F1: {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ceb6fc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, X.shape[1])\n",
    "traced_model = torch.jit.trace(model, dummy_input)\n",
    "traced_model.save(\"churn_ann.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "churn-prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
